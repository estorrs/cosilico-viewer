from collections.abc import Iterable
from collections import defaultdict
from pathlib import Path
from typing import Annotated, Union
import os

from scipy.sparse import issparse, coo_matrix
import numcodecs
import numpy as np
import pandas as pd
import zarr

from cosilico_py.preprocessing.core.tiling import (
    generate_tiled_data_grouped, compute_grid_centroids_multi
)
from cosilico_py.models import Layer, LayerMetadata
from cosilico_py.ports.anndata import AnnData

def append_missing(vals_df, fnames, fill_value=0):
    """
    Populates missing values.
    """
    missing = list(set(np.arange(len(fnames))) - set(vals_df.index.to_list()))
    tail = pd.DataFrame(np.full((len(missing), 1), fill_value, dtype=vals_df.values.dtype), columns=vals_df.columns, index=missing)
    vals_df = pd.concat((vals_df, tail)).sort_index()
    return vals_df

def get_values_meta(df, col, fnames, index_col='feature_index'):
    """
    Get vmin and vmax metadata for the given column, grouped by feature name.
    """
    vmaxs = df[[index_col, col]].groupby(index_col, observed=False).max()
    vmaxs = append_missing(vmaxs, fnames, fill_value=0)
    vmaxs[pd.isnull(vmaxs)] = 0

    vmins = df[[index_col, col]].groupby(index_col, observed=False).min()
    vmins = append_missing(vmins, fnames, fill_value=0)
    vmins[pd.isnull(vmins)] = 0
    
    return vmins.values.flatten().astype(np.float32), vmaxs.values.flatten().astype(np.float32)

def generate_zoom_dfs_grouped(
        source: Annotated[pd.DataFrame, 'Dataframe to generate zoom dataframes from. Should have x_location, y_location, feature_index columns.'],
        id_col: Annotated[str, 'Column to use as an ID in source, should be unique for every row in source.'],
        centroids_dfs: Annotated[dict[str, pd.DataFrame], 'Should be a map of centroid dataframes returned from cosilico_py.preprocessing.core.tiling.compute_grid_centroids_multi.'],
        zooms: Annotated[Iterable[int], 'The zoom resolutions the centroid dataframes apply to.'],
        bin_size_map: Annotated[dict[int, float], 'Bin sizes to use for each zoom.'],
        zoom_n_per_group: Annotated[Iterable[int], 'Group size to use when binning data for each zoom. Should be same length as zooms.'],
    ) -> Annotated[dict[int, dict[str, dict[str, pd.DataFrame]]], 'Map of each zoom to a series of sub dataframes organized by zoom, grid, and group.']:
    index_col='feature_index'

    assert len(zooms) == len(zoom_n_per_group)

    zoom_to_df = {}
    for i, zoom in enumerate(zooms):
        if i == 0:
            ## need to check this to see if the .copy is necessary.
            df = generate_tiled_data_grouped(
                source.copy(),
                n_per_group=zoom_n_per_group[i],
                grid_size=zoom
            )
            df['id'] = df[id_col].astype(str) if id_col is not None else np.arange(
                0, df.shape[0], dtype=object)
        else:
            bin_size = bin_size_map[zoom]
            centroid_df = centroids_dfs[bin_size]
            df = generate_tiled_data_grouped(
                centroid_df,
                n_per_group=zoom_n_per_group[i],
                grid_size=zoom
            )
            gds, _ = zip(*df.index.to_list())
            df['id'] = [
                f'{grid}_{fidx}_{bx}_{by}'
                for grid, (fidx, bx, by) in zip(gds, df[[index_col, 'bin_x', 'bin_y']].values)
            ]

        zoom_to_df[zoom] = df

    zoom_to_sub_dfs = {}
    for zoom, df in zoom_to_df.items():
        zoom_to_sub_dfs[zoom] = {}
        for (grid, group), sub_df in df.groupby(["grid", "group"], observed=False):
            if grid not in zoom_to_sub_dfs[zoom]:
                zoom_to_sub_dfs[zoom][grid] = {}

            zoom_to_sub_dfs[zoom][grid][group] = sub_df

    return zoom_to_df, zoom_to_sub_dfs

def write_points_zarr_grouped(
        source: Annotated[pd.DataFrame, 'Dataframe to generate zoom dataframes from. Should have x_location, y_location, feature_index columns.'],
        zooms: Annotated[Iterable[int], 'The zoom resolutions to be used.'],
        bin_sizes: Annotated[Iterable[int], 'Bin sizes to use for each zoom.'],
        zoom_to_df: Annotated[dict[str, pd.DataFrame], 'Dictionary mapping bin size to information on each centroid. Generated by cosilico_py.preprocessing.core.layer.generate_zoom_dfs_grouped.'],
        zoom_to_sub_dfs: Annotated[dict[int, dict[str, dict[str, pd.DataFrame]]], 'Map of each zoom to a series of sub dataframes organized by zoom, grid, and group. Generated by cosilico_py.preprocessing.core.layer.generate_zoom_dfs_grouped.'],
        zarr_path: Annotated[os.PathLike, 'Filepath to write output .zarr.zip file.'],
        size: Annotated[Iterable[int], 'Size of the image area the features are tied to. (H, W)'],
        version: Annotated[str, 'Version of Layer we are writing.'] = 'v1',
        name: Annotated[str, 'Name of the Layer.'] = 'Features',
    ) -> None:

    id_col='id'
    store = zarr.storage.ZipStore(zarr_path, mode='w')
    root = zarr.group(store=store, overwrite=True)

    root.attrs.update({
        'version': version,
        'name': name,
        'resolutions': zooms,
        'bin_sizes': bin_sizes,
        'size': list(size),
        'type': {zoom: 'point' for zoom in zooms}
    })
    
    metadata_root = root.create_group("metadata")
    feature_meta_group = metadata_root.create_group("features")

    names = source['feature_name'].cat.categories.to_numpy(dtype=object)
    fnames = feature_meta_group.create_dataset("feature_names", shape=(len(names),), chunks=(len(names),), dtype=object, object_codec=numcodecs.VLenUTF8())
    fnames[:] = names

    fg_group = feature_meta_group.create_group('feature_groups')


    ids_group = metadata_root.create_group("ids")
    

    zoom_root = root.create_group("zooms")
    for zoom, sub_dfs in zoom_to_sub_dfs.items():
        zoom_group = zoom_root.create_group(str(zoom))
        
        group_to_features = zoom_to_df[zoom].groupby("group", observed=False)["feature_index"].agg(set).to_dict()
        group_to_feature_names = {k:[source['feature_name'].cat.categories[v] for v in vs] for k, vs in group_to_features.items()}
        feature_to_group = {feat:g for g, feats in group_to_feature_names.items() for feat in feats}
        fgroups = fg_group.create_dataset(str(zoom), shape=(len(names),), chunks=(len(names),), dtype=np.int32)
        fgroups[:] = np.asarray([feature_to_group.get(x, -1) for x in names], dtype=np.int32)

        block_ids = []
    
        for grid, d in sub_dfs.items():
            grid_group = zoom_group.create_group(grid)
            for group_idx, f in d.items():
                block_idxs = np.arange(len(block_ids), len(block_ids) + f.shape[0])
                block_ids += f[id_col].to_list()

                channel_group = grid_group.create_group(str(group_idx))

                location = channel_group.create_dataset("location", shape=(f.shape[0], 2), chunks=(f.shape[0], 1), dtype='float32')
                location[:] = f[['x_location', 'y_location']].values

                fidxs = channel_group.create_dataset("feature_index", shape=(f.shape[0],), chunks=(f.shape[0],), dtype='uint32')
                fidxs[:] = f[['feature_index']].values.flatten()

                transcript_id = channel_group.create_dataset("id", shape=(f.shape[0],), chunks=(f.shape[0],), dtype=object, object_codec=numcodecs.VLenUTF8())
                transcript_id[:] = f[[id_col]].astype(object).values.flatten()

                transcript_idx = channel_group.create_dataset("id_idxs", shape=(f.shape[0],), chunks=(f.shape[0],), dtype='uint32')
                transcript_idx[:] = block_idxs

        ids = ids_group.create_dataset(str(zoom), shape=(len(block_ids),), dtype=object, object_codec=numcodecs.VLenUTF8())
        ids[:] = np.asarray(block_ids, dtype=object)

    store.close()


def write_grouped_layer_zarr_from_df(
        experiment_id: Annotated[str, 'ID of the parent experiment.'],
        source: Annotated[pd.DataFrame, 'Dataframe to generate zoom dataframes from. Should have x_location, y_location, feature_index columns.'],
        id_col: Annotated[str, 'Column to use as an ID in source, should be unique for every row in source.'],
        zooms: Annotated[Iterable[int], 'The zoom resolutions to be used.'],
        bin_size_map: Annotated[dict[int, int], 'Bin sizes to use for each zoom.'],
        group_sizes: Annotated[Iterable[int], 'Group sizes to use for each zoom.'],
        output_dir: Annotated[os.PathLike, 'Directory in which to write the .zarr.zip file.'],
        size: Annotated[Iterable[int], 'Size of the image area the features are tied to. (H, W)'],
        version: Annotated[str, 'Version of Layer we are writing.'] = 'v1',
        name: Annotated[str, 'Name of the Layer.'] = 'Features',
        chunk_size: Annotated[int, 'Chunk size to use when batch processing.'] = 10_000_000,
        use_disk: Annotated[bool, 'Whether to write batch files to disk to decrease memory usage. Default is False'] = True,
    ) -> Annotated[Layer, 'The resulting Layer object.']:
    output_directory = Path(output_dir).expanduser().absolute()
    assert output_directory.is_dir(), f'{output_dir} is not a directory.'

    centroids_dfs = compute_grid_centroids_multi(
        source[['feature_index', 'x_location', 'y_location']],
        bin_sizes=list(bin_size_map.values()), chunk_size=chunk_size,
        use_disk=use_disk
    )

    zoom_to_df, zoom_to_sub_dfs = generate_zoom_dfs_grouped(source, id_col, centroids_dfs, zooms, bin_size_map, group_sizes)

    layer = Layer(
        name=name,
        experiment_id=experiment_id,
        is_grouped=True,
    )
    layer.local_path = (output_directory / f'{layer.id}.zarr.zip').absolute()

    write_points_zarr_grouped(source, zooms, [bin_size_map.get(k) for k in zooms], zoom_to_df, zoom_to_sub_dfs, layer.local_path, size, version=version, name=name)

    return layer




# grouped layer metadata zar
def generate_layer_metadata_zoom_to_df(
        source: Annotated[pd.DataFrame, 'Dataframe to generate zoom dataframes from. Should have x_location, y_location, feature_index columns. Additionally, there should be a variable_name column that determines the group, and value_cols for continuous variables we want means for.'],
        id_col: Annotated[str, 'Column to use as an ID in source, should be unique for every row in source.'],
        parent_attrs: Annotated[dict, 'Zarr attrs from the parent grouped layer.'],
        bin_size_map: Annotated[dict[int, int], 'Bin sizes to use for each zoom.'],
        fnames: Annotated[Iterable[str], 'The order of fields that are in the variable_name column.'],
        variable_name: Annotated[str, 'The column in source containing fnames. This is the variable that the parent layer was grouped on.'],
        value_cols: Annotated[Iterable[str], 'Continuous variables in the source dataframe that should be aggregated.'] = None,
        chunk_size: Annotated[int, 'Chunk size to use when batch processing.'] = 10_000_000,
        use_disk: Annotated[bool, 'Whether to write batch files to disk to decrease memory usage. Default is False'] = True,
    ):
    if value_cols is None:
        value_cols = []

    bin_sizes = [x for x in parent_attrs['bin_sizes'] if x is not None]
    feat_to_index = {x:i for i, x in enumerate(fnames)}
    
    source['feature_index'] = source[variable_name].map(feat_to_index)
    
    centroids_dfs = compute_grid_centroids_multi(
        source[['feature_index', 'x_location', 'y_location'] + value_cols],
        bin_sizes=bin_sizes,
        chunk_size=chunk_size,
        use_disk=use_disk,
        target_columns=value_cols
    )

    zoom_to_df, _ = generate_zoom_dfs_grouped(
        source,
        id_col,
        centroids_dfs,
        parent_attrs['resolutions'],
        bin_size_map,
        [len(feat_to_index)] * len(parent_attrs['resolutions'])
    )

    return zoom_to_df

def write_grouped_metadata_zarr(
        zoom_to_df: Annotated[dict[str, pd.DataFrame], 'Dictionary mapping bin size to information on each centroid. Generated by cosilico_py.preprocessing.core.layer.generate_layer_metadata_zoom_to_df.'],
        zarr_path: Annotated[os.PathLike, 'Filepath to write output .zarr.zip file.'],
        name: Annotated[str, 'Name of the Layer.'],
        feat_type: Annotated[str, 'Type of feature. Can be categorical or continuous.'],
        fnames: Annotated[Iterable[str], 'The order of fields.'],
        parent_attrs: Annotated[dict, 'Zarr attrs from the parent grouped layer.'],
        value_col: Annotated[Iterable[str], 'Continuous variable in the source dataframe that should be aggregated.'],
        version: Annotated[str, 'Version of Layer we are writing.'] = 'v1',
        absolute_vmin: Annotated[Union[float, None], 'Absolute vmin for a continuous field.'] = None,
        absolute_vmax: Annotated[Union[float, None], 'Absolute vmax for a continuous field.'] = None,
    ):
    assert feat_type in ['categorical', 'continuous'], f'feat_type must be "categorical" or "continuous", got {feat_type}'

    store = zarr.storage.ZipStore(zarr_path, mode='w')
    root = zarr.group(store=store, overwrite=True)
    
    metadata_root = root.create_group("metadata")

    xs = metadata_root.create_dataset('categories', shape=(len(fnames),), chunks=(len(fnames),), dtype=object, object_codec=numcodecs.VLenUTF8())
    xs[:] = np.asarray(fnames, dtype=object)

    object_root = root.create_group("object")
    vmin_stack = []
    vmax_stack = []
    for zoom in parent_attrs['resolutions']:
        df = zoom_to_df[zoom]
        
        if value_col not in df.columns:
            df[value_col] = 1
        
        xs = object_root.create_dataset(str(zoom), shape=(df.shape[0],), chunks=50_000, dtype='uint32' if feat_type == 'categorical' else 'float32')    
        xs[:] = df[value_col].values
        
        if feat_type == 'continuous':
            vmins_arr, vmaxs_arr = get_values_meta(df, value_col, fnames)
            vmin_stack.append(vmins_arr)
            vmax_stack.append(vmaxs_arr)
            
    if feat_type == 'continuous':
        vmin_stack, vmax_stack = np.stack(vmin_stack), np.stack(vmax_stack)
        vmins_arr, vmaxs_arr = vmin_stack.min(0), vmax_stack.max(0)

        if absolute_vmin is not None:
            vmins_arr = np.full_like(vmins_arr, absolute_vmin)
        if absolute_vmax is not None:
            vmaxs_arr = np.full_like(vmaxs_arr, absolute_vmax)

        vmins = metadata_root.create_dataset('vmins', shape=(len(fnames),), chunks=(len(fnames),), dtype='float32')
        vmaxs = metadata_root.create_dataset('vmaxs', shape=(len(fnames),), chunks=(len(fnames),), dtype='float32')
        vmaxs[:] = vmaxs_arr
        vmins[:] = vmins_arr
        
        shape = (len(parent_attrs['resolutions']), len(fnames),)
        vmins_by_res = metadata_root.create_dataset('vmins_by_res', shape=shape, chunks=shape, dtype='float32')
        vmaxs_by_res = metadata_root.create_dataset('vmaxs_by_res', shape=shape, chunks=shape, dtype='float32')
        vmins_by_res[:] = vmin_stack
        vmaxs_by_res[:] = vmax_stack
                
        abs_vmax = float(vmaxs_arr.max())
        abs_vmin = float(vmins_arr.min())
    else:
        abs_vmax = None
        abs_vmin = None

    root.attrs.update({
        'version': version,
        'name': name,
        'type': feat_type,
        'vmax': abs_vmax,
        'vmin': abs_vmin
    })

    store.close()

def write_grouped_metadata_zarrs_from_df(
        layer_id: Annotated[str, 'ID of the layer these metadata will be attached to.'],
        source: Annotated[pd.DataFrame, 'Dataframe to generate zoom dataframes from. Should have x_location, y_location, feature_index columns. Additionally, there should be a variable_name column that determines the group, and value_cols for continuous variables we want means for.'],
        id_col: Annotated[str, 'Column to use as an ID in source, should be unique for every row in source.'],
        output_dir: Annotated[os.PathLike, 'Directory where file output will be written.'],
        bin_size_map: Annotated[dict[int, int], 'Bin sizes to use for each zoom.'],
        fnames: Annotated[Iterable[str], 'The order of fields.'],
        parent_attrs: Annotated[dict, 'Zarr attrs from the parent grouped layer.'],
        zoom_to_order: Annotated[dict[int, Iterable], 'Zoom to ID order from the parent layer.'],
        value_params: Annotated[dict[str, dict], 'Keys are values to write from source. They should be continuous variables. The param dict should have name, vmin and vmax. Those entries should be set to null if it is not to be used.'],
        variable_name: Annotated[str, 'The column in source containing fnames. This is the variable that the parent layer was grouped on.'],
        chunk_size: Annotated[int, 'Chunk size to use when batch processing.'] = 10_000_000,
        use_disk: Annotated[bool, 'Whether to write batch files to disk to decrease memory usage. Default is False'] = True,
        version: Annotated[str, 'Version of Layer we are writing.'] = 'v1',
    ) -> Annotated[dict[str, LayerMetadata], 'The resulting LayerMetadata objects for the written zarrs.']:
    output_directory = Path(output_dir).expanduser().absolute()
    assert output_directory.is_dir(), f'{output_dir} is not a directory.'
    value_cols = list(value_params.keys())

    to_metadatas = {}

    feat_type = 'continuous' # for grouped layers all metadata needs to be continuous for now.
    feat_meta = source[[id_col, 'x_location', 'y_location', variable_name] + value_cols].copy()

    zoom_to_df = generate_layer_metadata_zoom_to_df(
        feat_meta, id_col, parent_attrs, bin_size_map, fnames, variable_name, value_cols=value_cols, chunk_size=chunk_size, use_disk=use_disk
    )

    for zoom, df in zoom_to_df.items():
        df = df.set_index('id')
        df.index = df.index.astype(str)
        df = df.loc[zoom_to_order[zoom]]
        zoom_to_df[zoom] = df

    # do counts
    meta = LayerMetadata(
        layer_id=layer_id,
        name='counts',
        metadata_type='continuous',
        is_sparse=True,
        fields=fnames.tolist(),
    )
    meta.local_path = (output_dir / f'{meta.id}.zarr.zip').absolute()
    
    write_grouped_metadata_zarr(zoom_to_df, meta.local_path, 'Count', feat_type, fnames, parent_attrs, 'count', version=version, absolute_vmin=0)
    to_metadatas['Count'] = meta

    # do value cols
    for value_col in value_cols:
        name, vmin, vmax = [value_params[value_col][x] for x in ['name', 'vmin', 'vmax']]

        meta = LayerMetadata(
            layer_id=layer_id,
            name=name,
            metadata_type='continuous',
            is_sparse=True,
            fields=[name],
        )
        meta.local_path = (output_dir / f'{meta.id}.zarr.zip').absolute()
        write_grouped_metadata_zarr(zoom_to_df, meta.local_path, name, feat_type, fnames, parent_attrs, value_col, version=version, absolute_vmin=vmin, absolute_vmax=vmax)
        to_metadatas[value_col] = meta
    
    return to_metadatas





# point and polygon dense layers
def extract_polygons_fast(
        df: Annotated[pd.DataFrame, 'Dataframe containing polygon info for features. Must have id_col, vertex_x, and vertex_y columns.'],
        id_col: Annotated[str, 'Column to use a the ID for a polygon'],
        x_col: Annotated[str, 'Column to use a the x vertex location in a polygon'] = 'vertex_x',
        y_col: Annotated[str, 'Column to use a the xyvertex location in a polygon'] = 'vertex_y',
        max_verts: Annotated[int, 'Will downsample polygon verts to this number if not None.'] = None
    ) -> Annotated[Iterable, 'Returns polygons and their ids']:
    """
    Extract polygons from a dataframe.
    If max_verts is provided, downsample polygons with more vertices.
    If all polygons are shorter than max_verts, the output shape matches the true max.
    """
    # Sort df by id_col for fast contiguous grouping
    df_sorted = df.copy()
    ids = df_sorted[id_col].to_numpy()
    x = df_sorted[x_col].to_numpy()
    y = df_sorted[y_col].to_numpy()

    # Identify group boundaries
    unique_ids, start_idx, counts = np.unique(ids, return_index=True, return_counts=True)

    n_cells = len(unique_ids)
    # Determine actual maximum length to use
    actual_max = counts.max()
    if max_verts is not None:
        final_len = min(max_verts, actual_max)
    else:
        final_len = actual_max

    polygons = np.full((n_cells, final_len, 2), -1, dtype=np.float32)

    for i, (start, count) in enumerate(zip(start_idx, counts)):
        coords = np.column_stack((x[start:start + count], y[start:start + count]))

        if max_verts is not None and count > max_verts:
            idx = np.linspace(0, count - 1, max_verts, dtype=int)
            coords = coords[idx]
        polygons[i, :len(coords), :] = coords

    return polygons, unique_ids

def get_zoom_to_subs(
        df: Annotated[pd.DataFrame, 'Dataframe containing polygon info for features. Must have id_col, vertex_x, and vertex_y columns.'],
        id_col: Annotated[str, 'ID column to use in df.'],
        zooms: Annotated[Iterable[int], 'Zoom level to gather polygons at.'],
        max_vert_map: Annotated[dict[int, int], 'Maps zoom level to max_verts for polygons at that zoom level.'],
        downsample_map: Annotated[dict[int, int], 'Maps zoom level to n polygons to downsample at that zoom level.'],
    ) -> Annotated[dict[int, pd.DataFrame], 'Gathered polygons for each grid in dataframe form.']:
    zoom_to_subs = {}
    for res in zooms:
        max_verts = max_vert_map[res]
        downsample = downsample_map[res]

        X, ids = extract_polygons_fast(df, id_col, max_verts=max_verts)
        if downsample > 0 and downsample < X.shape[0] - 1:
            idxs = np.linspace(0, X.shape[0] - 1, downsample, dtype=int)
            X, ids = X[idxs], ids[idxs]

        # Compute grid bins for all vertices of all polygons
        all_x_bins = (X[:, :, 0] // res).astype(int)
        all_y_bins = (X[:, :, 1] // res).astype(int)
        all_grids = np.core.defchararray.add(
            all_x_bins.astype(str), 
            "_" + all_y_bins.astype(str)
        )

        zoom_grid_map = defaultdict(lambda: {"X": [], "ids": []})

        for i, cell_id in enumerate(ids):
            grids = np.unique(all_grids[i])
            for grid in grids:
                # print('grid', grid)
                zoom_grid_map[grid]["X"].append(X[i])
                zoom_grid_map[grid]["ids"].append(cell_id)

        grid_to_info = {}
        for grid, data in zoom_grid_map.items():
            grid_to_info[grid] = {
                "X": np.stack(data["X"]),
                "ids": data["ids"]
            }
        
         # get rid of negative bins as result of padding
        grid_to_info = {k:v for k, v in grid_to_info.items() if '-' not in k}

        zoom_to_subs[res] = grid_to_info

    return zoom_to_subs


def write_ungrouped_layer_zarr(
        zoom_to_subs: Annotated[dict[int, pd.DataFrame], 'Gathered polygons for each grid in dataframe form. From cosilico_py.preprocessing.core.layer.get_zoom_to_subs.'],
        zarr_path: Annotated[os.PathLike, 'Filepath to write output .zarr.zip file.'],
        name: Annotated[str, 'Name of Layer.'],
        object_type_map: Annotated[dict[int, str], 'Maps object type (point or polygon), to zoom level.'],
    ):
    store = zarr.storage.ZipStore(zarr_path, mode='w')
    root = zarr.group(store=store, overwrite=True)

    resolutions = sorted(zoom_to_subs.keys())

    root.attrs.update({
        'version': 'v1',
        'name': name,
        'resolutions': resolutions,
        'object_types': [object_type_map[x] for x in resolutions],
    })
    
    metadata_root = root.create_group("metadata")
    ids_group = metadata_root.create_group("ids")

    zoom_root = root.create_group("zooms")
    for zoom, grid_to_info in zoom_to_subs.items():
        zoom_group = zoom_root.create_group(str(zoom))

        block_ids = []        
        for grid, info in grid_to_info.items():
            grid_group = zoom_group.create_group(grid)
            block_idxs = np.arange(len(block_ids), len(block_ids) + len(info['ids']))
            block_ids += info['ids']

            xs = grid_group.create_dataset("id", shape=(len(info['ids']),), chunks=(len(info['ids']),), dtype=object, object_codec=numcodecs.VLenUTF8())
            xs[:] = np.asarray(info['ids'], object)

            xs = grid_group.create_dataset("id_idxs", shape=(len(info['ids']),), chunks=(len(info['ids']),), dtype='uint32')
            xs[:] = block_idxs

            xs = grid_group.create_dataset("vertices", shape=info['X'].shape, chunks=info['X'].shape, dtype='float32')
            xs[:] = info['X']

        ids = ids_group.create_dataset(str(zoom), shape=(len(block_ids),), chunks=(len(block_ids),), dtype=object, object_codec=numcodecs.VLenUTF8())
        ids[:] = np.asarray(block_ids, dtype=object)

    store.close()

def write_ungrouped_layer_zarr_from_df(
        experiment_id: Annotated[str, 'ID of the parent experiment.'],
        name: Annotated[str, 'Name of the Layer.'],
        source: Annotated[pd.DataFrame, 'Dataframe containing polygon info for features. Must have id_col, vertex_x, and vertex_y columns.'],
        id_col: Annotated[str, 'Column to use as an ID in source, should be unique for every row in source.'],
        zooms: Annotated[Iterable[int], 'The zoom resolutions to be used.'],
        output_dir: Annotated[os.PathLike, 'Directory in which to write the .zarr.zip file.'],
        max_vert_map: Annotated[dict[int, int], 'Maps zoom level to max_verts for polygons at that zoom level.'],
        downsample_map: Annotated[dict[int, int], 'Maps zoom level to n polygons to downsample at that zoom level.'],
        object_type_map: Annotated[dict[int, str], 'Maps object type (point or polygon), to zoom level.'],
    ) -> Annotated[Layer, 'The resulting Layer object.']:
    output_directory = Path(output_dir).expanduser().absolute()
    assert output_directory.is_dir(), f'{output_dir} is not a directory.'

    zoom_to_subs = get_zoom_to_subs(source, id_col, zooms, max_vert_map, downsample_map)

    layer = Layer(
        name=name,
        experiment_id=experiment_id,
        is_grouped=False,
    )
    layer.local_path = (output_dir / f'{layer.id}.zarr.zip').absolute()

    write_ungrouped_layer_zarr(zoom_to_subs, layer.local_path, name, object_type_map)

    return layer




# non-grouped metadata
def combine_barcoded_data(
        spatial_df: Annotated[pd.DataFrame, "DataFrame containing 'barcode', 'x_location', 'y_location'."],
        adata: Annotated[AnnData, 'Anndata ported container.'],
        chunk_size: Annotated[int, 'How many rows to process at a time'] = 100_000
    ) -> Annotated[pd.DataFrame, "DataFrame with 'barcode', 'feature_name', 'x_location', 'y_location', 'count'."]:
    """
    Combines spatial locations with count information.
    """
    # Convert AnnData matrix to sparse CSC format if needed (efficient column slicing)
    if not issparse(adata.X):
        adata.X = coo_matrix(adata.X)  # Convert dense matrix to sparse COO format
    
    X = adata.X.tocsc()  # Convert to CSC for fast feature-wise slicing

    # Convert barcodes and feature names to arrays
    barcodes = np.array(adata.obs.index, dtype=str)  # Ensure barcode index is a NumPy string array
    features = np.array(adata.var.index, dtype=str)  # Ensure feature index is a NumPy string array

    # Preallocate results list
    results = []

    # Process data in chunks
    for start in range(0, X.shape[0], chunk_size):
        end = min(start + chunk_size, X.shape[0])

        # Extract a chunk of the barcode indices
        barcode_chunk = barcodes[start:end]

        # Convert to COO to extract nonzero values efficiently
        X_chunk = X[start:end].tocoo()

        # Extract nonzero indices and values
        barcode_indices = X_chunk.row
        feature_indices = X_chunk.col
        transcript_counts = X_chunk.data  # Nonzero counts

        # Convert indices to actual barcode and feature names
        barcode_values = barcode_chunk[barcode_indices]  # Fast slicing
        feature_values = features[feature_indices]  # Fast slicing

        # Create DataFrame for transcript counts (memory-efficient dtypes)
        counts_df = pd.DataFrame({
            "barcode": barcode_values,
            "feature_name": feature_values,
            "count": transcript_counts.astype(np.uint16)  # Reduce memory footprint
        }).set_index('barcode')
        
        if spatial_df is not None:
            # Merge spatial data with counts using an efficient join
            merged_df = counts_df.merge(spatial_df, right_index=True, left_index=True, how="inner")

            # Store result in list (avoids large intermediate DataFrames)
            results.append(merged_df)
        else:
            results.append(counts_df)

    # Concatenate final results and reset index
    final_df = pd.concat(results)
    final_df['feature_name'] = final_df['feature_name'].astype(str).astype("category")
    final_df['feature_index'] = pd.Categorical(final_df["feature_name"].cat.codes)

    return final_df

def get_zoom_to_sparse_dfs(
        source: Annotated[pd.DataFrame, 'Source dataframe.'],
        root: Annotated[zarr.Group, 'Zarr group from parent']
    ) -> dict:
    zoom_group = root['/zooms']
    zoom_to_dfs = {}

    source_index = source.index

    for zoom, group in zoom_group.groups():
        zoom_to_dfs[zoom] = {}

        for grid, grid_group in group.groups():
            ids = grid_group['id']
            mask = source_index.isin(ids)
            f = source.loc[mask]
            zoom_to_dfs[zoom][grid] = f
    return zoom_to_dfs

def write_sparse_continuous_metadata_zarr(
        zoom_to_dfs: Annotated[dict, 'Result of get_zoom_to_sparse_dfs.'],
        fnames: Annotated[Iterable[str], 'Field names for Fields in variable group.'],
        value_col: Annotated[str, 'column to use in dataframe for value.'],
        zarr_path: Annotated[os.PathLike, 'Filepath to write output .zarr.zip file.'],
        name: Annotated[str, 'Name of variable.'],
        vmins: Annotated[Iterable, 'vmin for fields in fnames, should be same length as fnames'],
        vmaxs: Annotated[Iterable, 'vmax for fields in fnames, should be same length as fnames'],
        vcenters: Annotated[Iterable, 'vcenter for fields in fnames, should be same length as fnames. If none no center will be used.'] = None
    ):
    assert len(fnames) == len(vmins), 'fnames and vmins should be the same length.'
    assert len(fnames) == len(vmaxs), 'fnames and vmaxs should be the same length.'

    store = zarr.storage.ZipStore(zarr_path, mode='w')
    root = zarr.group(store=store, overwrite=True)

    root.attrs.update({
        'version': 'v1',
        'name': name,
        'type': 'continuous',
        'is_sparse': True
    })

    metadata_root = root.create_group("metadata")
    s = (len(fnames),)
    xs = metadata_root.create_dataset('fields', shape=s, chunks=s, dtype=object, object_codec=numcodecs.VLenUTF8())
    xs[:] = np.asarray(fnames, dtype=object)
    
    xs = metadata_root.create_dataset('vmins', shape=s, chunks=s, dtype='float32')
    xs[:] = vmins
    
    xs = metadata_root.create_dataset('vmaxs', shape=s, chunks=s, dtype='float32')
    xs[:] = vmaxs
    
    xs = metadata_root.create_dataset('vcenters', shape=s, chunks=s, dtype='float32')
    if vcenters is None:
        xs[:] = np.full_like(vmaxs, -99999, dtype=np.float32)
    

    zoom_root = root.create_group("zooms")
    for zoom, d in zoom_to_dfs.items():
        group = zoom_root.create_group(str(zoom))
        for grid, df in d.items():
            grid_group = group.create_group(grid)
        
            xs = grid_group.create_dataset('ids', shape=df.shape[0], chunks=df.shape[0], dtype=object, object_codec=numcodecs.VLenUTF8())    
            xs[:] = np.asarray(df.index.astype(object).to_list())
            
            xs = grid_group.create_dataset('values', shape=df.shape[0], chunks=df.shape[0], dtype='float32')    
            xs[:] = df[value_col].values
            
            xs = grid_group.create_dataset('feature_indices', shape=df.shape[0], chunks=df.shape[0], dtype='uint32')    
            xs[:] = df['feature_index'].astype(int).values
    
    store.close()

def write_categorical_metadata_zarr(
        zoom_to_values: Annotated[dict[int, Iterable], 'Maps zoom level to values for each zoom level.'],
        fnames: Annotated[Iterable[str], 'Field names for Fields in variable group.'],
        zarr_path: Annotated[os.PathLike, 'Filepath to write output .zarr.zip file.'],
        name: Annotated[str, 'Name of variable.'],
    ):
    store = zarr.storage.ZipStore(zarr_path, mode='w')
    root = zarr.group(store=store, overwrite=True)
        
    root.attrs.update({
        'version': 'v1',
        'name': name,
        'type': 'categorical',
        'is_sparse': False
    })
    
    metadata_root = root.create_group("metadata")
    xs = metadata_root.create_dataset('fields', shape=(len(fnames),), chunks=(len(fnames),), dtype=object, object_codec=numcodecs.VLenUTF8())
    xs[:] = np.asarray(fnames, object)

    object_group = root.create_group("object")
    for zoom, values in zoom_to_values.items():
        xs = object_group.create_dataset(str(zoom), shape=(len(values),), chunks=50_000, dtype='uint32') 
        xs[:] = values
    
    store.close()

def write_continuous_metadata_zarr(
        zoom_to_df: Annotated[dict[int, pd.DataFrame], 'Zoom to dataframe where each column is a variable in the variable group.'],
        fnames: Annotated[Iterable[str], 'Field names for Fields in variable group. In this case fields correspond to a variable in the variable group.'],
        zarr_path: Annotated[os.PathLike, 'Filepath to write output .zarr.zip file.'],
        name: Annotated[str, 'Name of variable group.'],
        vmins: Annotated[Iterable, 'vmin for fields in fnames, should be same length as fnames'],
        vmaxs: Annotated[Iterable, 'vmax for fields in fnames, should be same length as fnames'],
        vcenters: Annotated[Iterable, 'vcenter for fields in fnames, should be same length as fnames. If none no center will be used.'] = None
    ):
    
    store = zarr.storage.ZipStore(zarr_path, mode='w')
    root = zarr.group(store=store, overwrite=True)
            
    root.attrs.update({
        'version': 'v1',
        'name': name,
        'type': 'continuous',
        'is_sparse': False,
    })
    
    metadata_root = root.create_group("metadata")
    s = (len(fnames),)
    xs = metadata_root.create_dataset('fields', shape=s, chunks=s, dtype=object, object_codec=numcodecs.VLenUTF8())
    xs[:] = np.asarray(fnames, dtype=object)
    
    xs = metadata_root.create_dataset('vmins', shape=s, chunks=s, dtype='float32')
    xs[:] = vmins
    
    xs = metadata_root.create_dataset('vmaxs', shape=s, chunks=s, dtype='float32')
    xs[:] = vmaxs
    
    xs = metadata_root.create_dataset('vcenters', shape=s, chunks=s, dtype='float32')
    if vcenters is None:
        xs[:] = np.full_like(vmaxs, -99999, dtype=np.float32)

        
    object_root = root.create_group("object")
    for zoom, df in zoom_to_df.items():
        xs = object_root.create_dataset(str(zoom), shape=df.shape, chunks=(10_000, df.shape[1]), dtype='float32')    
        xs[:] = df.values
    
    store.close()

def write_sparse_continuous_ungrouped_layer_metadata(
        layer_id: Annotated[str, 'ID of the parent layer.'],
        fnames: Annotated[Iterable[str], 'Field names for Fields in variable group. In this case fields correspond to a variable in the variable group.'],
        name: Annotated[str, 'Name of the Layer Metadata.'],
        source: Annotated[pd.DataFrame, 'Result of cosilico_py.preprocessing.core.layer.combine_barcoded_data.'],
        value_col: Annotated[str, 'column to use in dataframe for value.'],
        parent_zarr_path: Annotated[str, 'Filepath to parent layer zarr.'],
        output_dir: Annotated[os.PathLike, 'Directory in which to write the .zarr.zip file.'],
    ) -> Annotated[Layer, 'The resulting Layer Metadata object.']:
    output_directory = Path(output_dir).expanduser().absolute()
    assert output_directory.is_dir(), f'{output_dir} is not a directory.'

    store = zarr.storage.ZipStore(parent_zarr_path, mode='r')
    root = zarr.group(store=store)
    zoom_to_dfs = get_zoom_to_sparse_dfs(source, root)
    store.close()

    vmaxs = source[['feature_name', value_col]].groupby('feature_name', observed=True).max().loc[fnames].values.flatten()
    vmins = np.full_like(vmaxs, 0)
    vcenters = None

    meta = LayerMetadata(
        layer_id=layer_id,
        name=name,
        metadata_type='continuous',
        is_sparse=True,
        fields=fnames.tolist(),
    )
    meta.local_path = (output_dir / f'{meta.id}.zarr.zip').absolute()

    write_sparse_continuous_metadata_zarr(
        zoom_to_dfs, fnames, value_col, meta.local_path, name, vmins, vmaxs, vcenters=vcenters
    )

    return meta

def write_categorical_ungrouped_layer_metadata(
        layer_id: Annotated[str, 'ID of the parent layer.'], 
        name: Annotated[str, 'Name of the Layer Metadata.'],
        values: Annotated[pd.Series, 'Values we are writing. IDs must match the IDs on the parent layer.'],
        parent_zarr_path: Annotated[str, 'Filepath to parent layer zarr.'],
        output_dir: Annotated[os.PathLike, 'Directory in which to write the .zarr.zip file.'],
    ) -> Annotated[Layer, 'The resulting Layer Metadata object.']:
    output_directory = Path(output_dir).expanduser().absolute()
    assert output_directory.is_dir(), f'{output_dir} is not a directory.'

    store = zarr.storage.ZipStore(parent_zarr_path, mode='r')
    root = zarr.group(store=store)
    attrs = root.attrs
    zoom_to_order = {res:root[f'/metadata/ids/{res}'][:] for res in attrs['resolutions']}
    store.close()

    values = values.astype("category")
    fnames = values.cat.categories.to_list()

    zoom_to_values = {}
    for zoom, id_order in zoom_to_order.items():
        xs = np.asarray(values.loc[id_order].cat.codes, dtype=np.uint32)
        zoom_to_values[zoom] = xs

    meta = LayerMetadata(
        layer_id=layer_id,
        name=name,
        metadata_type='categorical',
        is_sparse=False,
        fields=fnames,
    )
    meta.local_path = (output_dir / f'{meta.id}.zarr.zip').absolute()

    write_categorical_metadata_zarr(
        zoom_to_values, fnames, meta.local_path, name
    )

    return meta

def write_continuous_ungrouped_layer_metadata(
        layer_id: Annotated[str, 'ID of the parent layer.'],
        name: Annotated[str, 'Name of the Layer Metadata.'],
        values_df: Annotated[pd.DataFrame, 'Dataframe with continuous variables we are writing for this variable group. Must be in the same order as the IDs on the parent layer and index must be entity IDs from the parent layer.'],
        parent_zarr_path: Annotated[str, 'Filepath to parent layer zarr.'],
        output_dir: Annotated[os.PathLike, 'Directory in which to write the .zarr.zip file.'],
    ) -> Annotated[Layer, 'The resulting Layer Metadata object.']:
    output_directory = Path(output_dir).expanduser().absolute()
    assert output_directory.is_dir(), f'{output_dir} is not a directory.'

    store = zarr.storage.ZipStore(parent_zarr_path, mode='r')
    root = zarr.group(store=store)
    attrs = root.attrs
    zoom_to_order = {res:root[f'/metadata/ids/{res}'][:] for res in attrs['resolutions']}
    store.close()

    fnames = list(values_df.columns)

    zoom_to_values_df = {}

    vmins, vmaxs = None, None
    for zoom, id_order in zoom_to_order.items():
        values_df = pd.DataFrame(values_df.loc[id_order].values, columns=fnames, index=id_order)
        zoom_to_values_df[zoom] = values_df
        
        if zoom == min(zoom_to_order.keys()):
            vmins = values_df.min(0).values
            vmaxs = values_df.max(0).values

    meta = LayerMetadata(
        layer_id=layer_id,
        name=name,
        metadata_type='continuous',
        is_sparse=False,
        fields=fnames,
    )
    meta.local_path = (output_dir / f'{meta.id}.zarr.zip').absolute()

    write_continuous_metadata_zarr(
        zoom_to_values_df, fnames, meta.local_path, name, vmins, vmaxs
    )

    return meta